\documentclass{easychair}

\usepackage{setspace}

\begin{document}

\title{Distributed programming via safe closure passing}

% \titlerunning{} has to be set to either the main title or its shorter
% version for the running heads. When processed by
% EasyChair, this command is mandatory: a document without \titlerunning
% will be rejected by EasyChair

\titlerunning{Safe closure passing}

% Authors are joined by \and. Their affiliations are given by \inst, which indexes
% into the list defined using \institute
%
\author{
  Philipp Haller\inst{1}
\and
  Heather Miller\inst{2}
}

% Institutes for affiliations are also joined by \and,
\institute{
  KTH Royal Institute of Technology,
  Sweden\\
  \email{phaller@kth.se}
\and
  EPFL,
  Switzerland\\
  \email{heather.miller@epfl.ch}\\
}

%  \authorrunning{} has to be set for the shorter version of the authors' names;
% otherwise a warning will be rendered in the running heads. When processed by
% EasyChair, this command is mandatory: a document without \authorrunning
% will be rejected by EasyChair

\authorrunning{Haller and Miller}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}

Programming systems incorporating aspects of functional programming, e.g.,
higher-order functions, are becoming increasingly popular for large-scale
distributed programming. New frameworks such as Apache Spark leverage
functional techniques to provide high-level, declarative APIs for in-memory
data analytics, often outperforming traditional ``big data'' frameworks like
Hadoop MapReduce. However, widely-used programming models remain rather
ad-hoc; aspects such as implementation trade-offs, static typing, and semantics
are not yet well-understood. We present a new asynchronous programming model
that has at its core several principles facilitating functional processing of
distributed data. The emphasis of our model is on simplicity, performance,
expressiveness, and a well-defined operational semantics. The primary means of
communication is by passing functions (closures) to distributed, immutable
data. To ensure safe and efficient distribution of closures, our model
leverages both syntactic and type-based restrictions. We report on a prototype
implementation as an embedded domain-specific language in Scala. Finally, we
present first results practically evaluating our implementation.

\end{abstract}

%------------------------------------------------------------------------------
\section{Introduction}
\label{sec:introduction}

Programming systems for large-scala data processing are increasingly embracing
functional programming, i.e., programming with first-class functions and
higher-order functions. Arguably, one of the first widely-used programming
models for ``big data'' processing making use of concepts from functional
programming is Google's MapReduce~\cite{DeanG08}. Indeed, \cite{Lammel08}
shows a precise executable semantics of MapReduce in Haskell. While leveraging
functional programming \emph{concepts}, popular implementations of the
MapReduce model, such as Hadoop MapReduce\footnote{See \url{http://hadoop.apache.org/}}
for Java, have been developed without making
use of functional \emph{language features} such as closures. In contrast, a new generation
of programming systems for large-scale data processing, such as
Apache Spark~\cite{Zaharia2012},
Twitter's Scalding\footnote{See \url{https://github.com/twitter/scalding/}},
and Scoobi\footnote{See \url{http://nicta.github.io/scoobi/}} build on functional
language features in order to provide high-level, declarative APIs.

However, these programming systems suffer from several problems that
negatively affect their usage, maintainance, and optimization:

\begin{itemize}

\item Their APIs cannot statically prevent \emph{common usage errors}. As a result, users are often
      confronted with runtime errors that are hard to debug. A common example is
      unsafe closure serialization~\cite{MillerHO14}.

\item Typically, only high-level user-facing abstractions are statically
      typed. Static types are quickly lost in deeper layers of the system,
      which complicates its \emph{maintainance}. For example, code refactorings
      are more error-prone.

% communicating messages with incorrect types can cause deadlocks

\item The absence of certain kinds of static type information precludes systems-centric \emph{optimizations}.
      Importantly, type-based static meta-programming enables fast serialization~\cite{MillerHBO13}, but this is
      only possible if also lower layers (namely those dealing with object serialization) are statically typed.
      Several studies~\cite{Carpenter1999,Maassen1999,Philippsen2000,Welsh2000} report on the high
      overhead of serialization in widely-used runtime environments such as the JVM. This overhead is so
      important in practice that popular systems, like Spark~\cite{Zaharia2012} and Akka~\cite{Akka},
      leverage alternative serialization frameworks such as Protocol Buffers~\cite{Protobuf} (Google),
      Apache Avro~\cite{Avro}, or Kryo~\cite{Kryo}.

\end{itemize}

\paragraph{Contributions}

This paper makes the following contributions:
\begin{itemize}

\item A new asynchronous programming model, called SCP (``safe closure passing''), for
      functional processing of distributed data (see Sec.~\ref{sec:model}). We propose to
      address the above problems through a novel combination of: (a) \emph{safe closures;}
      to prevent common usage errors. Closures that are not guaranteed to be serializable
      are rejected at compile time; (b) \emph{a statically-typed implementation of a generic
      distributed, persistent data structure.} Preserving static types through more system
      layers improves maintainability and enables the optimization of performance-critical
      parts, such as serialization.

\item An implementation of the SCP model in Scala (see Sec.~\ref{sec:impl}). In addition,
      we report on preliminary experimental experience using SCP to evaluate the end-to-end
      performance impact of a type-based optimization of serialization.

\end{itemize}

An important goal of our model is to better understand programming systems such as Spark, Scalding, and
Scoobi, by incorporating several of their core principles; namely, immutable
distributed data and distributed closure passing. By focussing on simplicity,
expressiveness, and performance (and ignoring many of the more ad-hoc
refinements of the mentioned programming models) our
programming model--together with its prototype implementation--enables exploring
implementation trade-offs, and capturing the semantics of the core constructs more
precisely.

% The primary means of communication is by passing functions (closures)
% to distributed, immutable data.

To ensure safe and efficient distribution of closures, our model leverages
both syntactic and type-based restrictions. For instance, closures sent to
remote nodes are required to conform to the restrictions imposed by the
so-called ``Spore'' abstraction that the authors presented in previous
work~\cite{MillerHO14}. Among others, the syntax and static semantics of
Spores can guarantee the absence of runtime serialization errors due to
closure environments that are not serializable.


\section{The SCP Programming Model}
\label{sec:model}

% Note to Heather: a lot of the following is copied from our draft at:
% https://docs.google.com/document/d/1T6yWFz0QxMPUMqXYFrTTtBkPb3iOaZdWOUGjjNBqKtY/edit#

The programming model has a few basic abstractions at its center: first, the
so-called \emph{silo}. A silo is a typed data container. It is stationary in the
sense that it does not move between machines. A silo remains on the machine
where it was created. Data stored in a silo is typically loaded from stable
storage, such as a distributed file system. A program operating on data stored
in a silo can only do so using a reference to the silo, a so-called \emph{SiloRef}.
Similar to a proxy object, a SiloRef represents, and allows interacting with,
a silo possibly located on a remote node. Some programming patterns require
combining data contained in silos located on different nodes (e.g., joins). To
support such patterns, our model includes a \emph{pump} primitive for emitting
data to silos on arbitrary nodes (explained further below).

A SiloRef has the following main operations:
\begin{verbatim}
trait SiloRef[T] {
  def apply(s: Spore[T, S]): SiloRef[S]
  def send(): Future[T]
}
\end{verbatim}

\paragraph{Apply}

The \verb|apply| method takes a spore, a kind of closure (see below for an overview), that is to be applied to
the data in the silo of the receiver SiloRef. Rather than immediately sending
the spore across the network, and waiting for the operation to finish, the
apply method is \emph{lazy}. It immediately returns a SiloRef that refers to the
result silo.

To realize something like the \verb|map| combinator of a (distributed) collection using \verb|apply|,
it is helpful to think of the Spore argument to \verb|apply| (``s'') as the composition of a
user-defined function passed to the \verb|map| combinator with the actual implementation of \verb|map|.

\paragraph{Example}

\begin{verbatim}
val ref: SiloRef[List[Int]] = ...
val userFun: Int => String = ...
val mapFun: (Int => String) => List[Int] => List[String] = ...
val ref2: SiloRef[List[String]] =
      ref.apply(mapFun(userFun))
\end{verbatim}

In the above example, the higher-order \verb|mapFun| function is expressed in curried
style where the user's function argument is passed as the first argument.
Applying \verb|mapFun| to a function of type \verb|Int => String| returns a function of type
\verb|List[Int] => List[String]|.

The result of invoking \verb|apply| is another SiloRef, which has a reference to the
Spore and the SiloRef that it was derived from. Note that this is semantically
the same as programming with normal functional data structures, where a new
data structure is defined by a transformation of an original data structure.

\paragraph{Send}

The \verb|send| method takes no argument, and returns a future. Unlike
\verb|apply|, \verb|send| is \emph{eager} (readers familiar with the concept
of \emph{views} might recognize a similarity to forcing a view). That is, it
sends whatever operations are queued up (by invocations of \verb|apply|) on a
given SiloRef to the node that contains the corresponding silo, and kicks of
the materialization of the result silo. Once the materialization is done, the
future returned by \verb|send| is completed.

\paragraph{Example}

\begin{verbatim}
val ref2 = ref1.apply(s)  // lazy
val fut  = ref2.send()    // eager
\end{verbatim}
\noindent
The invocation of \verb|send| kicks off the following sequence of actions:
\begin{enumerate}
\item A ``send'' control message is sent to the node where ref2's silo is located.
\item Since \verb|ref2| is derived from \verb|ref1|, \verb|ref1|'s silo is located on the same node. Thus, the runtime demands
      \verb|ref1| to be materialized; once this is done, spore \verb|s| is applied, populating \verb|ref2|'s silo (on the same node).
\item Once \verb|ref2|'s silo is materialized, its data is sent to the master node, completing the \verb|fut| future.
\end{enumerate}
\noindent
Note that since a \verb|send| operation sends the data of a silo to the master
node, it should only be invoked on silos containing small bits of data.

\paragraph{pumpTo}

The SiloRef singleton object provides an additional method for combining silos
storing collections:

\begin{verbatim}
def pumpTo[T <: Traversable[U], V, R](
      p: Place,
      silo1: SiloRef[T],
      silo2: SiloRef[T],
      fun: Spore[(U, Emitter[V]), Unit],
      bf: BuilderFactory[V, R]): SiloRef[R]
\end{verbatim}
\noindent
The \verb|pumpTo| method requires the silos silo1 and silo2 to contain collections of
element type U (Traversable[U]). Using pumpTo, the elements (of type U) of the
two silos are passed one-by-one to the user-provided Spore fun. This Spore
takes a pair as argument containing two components: first, a single element of
one of the silo's collections, and second, an emitter to which the Spore can
output values of type V. By emitting such elements, a new silo at the
destination (Place p) is filled, yielding a collection of type R. A
BuilderFactory[V, R] provides the functionality for building a collection of
type R based on elements of type V. Finally, pumpTo returns the SiloRef of the
silo that was created at Place p.

Although conceptually related, the Emitter and the BuilderFactory address two
separate issues: the Emitter provides a way to output elements from the source
silo; the BuilderFactory provides a way to input data into a newly created
silo at the destination. Both abstractions are explained in more detail below.

An Emitter is a simple trait which allows the Spore parameter fun of pumpTo to
emit zero, one, or multiple values per element of a silo's collection, using an
\verb|emit| function:
\begin{verbatim}
def emit(v: T)(implicit p: Pickler[T]): Unit
\end{verbatim}
\noindent
Note that Emitter differs from the well-known Observable abstraction~\cite{Meijer12}
in one important way: the emit method requires an implicit type-specific
pickler. In Scala, type-specialized picklers enable very fast serialization
through compile-time meta-programming~\cite{MillerHBO13}. Thus, Emitter is an
abstraction specially designed for distributed programming. The main reason
why the pickler is required already at this point is that we'd like to enable
picklers to be \emph{specialized} to the type of the pickled values. However,
this means a pickler has to be constructed at the point when the complete type
information of the emitted values is still available (essentially, before the
value loses its type when treated as generic data to be sent across the
network).

\subsection{Combining multiple silos}

Operations on distributed collections such as union, groupByKey, or join,
involve multiple data sets, possibly located on different nodes. In the
following we explain how such operations can be expressed using the introduced
primitives.

\paragraph{union}

The union of two unordered collections stored in two different silos can be
expressed directly using the above pumpTo primitive.

\paragraph{join}

Suppose we are given two silos with the following types:
\begin{verbatim}
val silo1: SiloRef[List[A]]
val silo2: SiloRef[List[B]]
\end{verbatim}
\noindent
as well as two hash functions computing hashes for elements of type A and B, respectively:
\begin{verbatim}
val hashA: A => K = ...
val hashB: B => K = ...
\end{verbatim}
\noindent
The goal is to compute the hash-join of silo1 and silo2:
\begin{verbatim}
val hashJoin: SiloRef[List[(K, (A, B))]] = ???
\end{verbatim}
\noindent
To be able to use pumpTo, the types of the two silos first have to be made equal, through initial applies:
\begin{verbatim}
  val silo12: SiloRef[List[(K, Option[A], Option[B])] =
        silo1.apply { x => (hashA(x), Some(x), None) }
  val silo22: SiloRef[List[(K, Option[A], Option[B])] =
        silo2.apply { x => (hashB(x), None, Some(x)) }
\end{verbatim}
\noindent
Then, we can use pumpTo to create a new silo (at some destination place), which contains the elements of both silo12 and silo22:
\begin{verbatim}
  val combined =
        SiloRef.pumpTo(destPlace, silo12, silo22,
                 (elem, emitter) => emitter.emit(elem),
                 listBuilderFactory[...])
\end{verbatim}
\noindent
The combined silo contains triples of type (K, Option[A], Option[B]). Using an
additional apply, the collection can be sorted by key, and adjacent triples be
combined, yielding finally a SiloRef[List[(K, (A, B))]] as required.

\paragraph{Partitioning and groupByKey}

A groupByKey operation on a group of silos containing collections needs to create multiple result silos, on each node, with ranges of keys supposed to be shipped to destination nodes. These destination nodes are determined using a partitioning function. Our goal, concretely:
\begin{verbatim}
val groupedSilos = groupByKey(silos)
\end{verbatim}
\noindent
Furthermore, we assume that silos.size = N where N is the number of nodes,
with nodes N1, N2, etc. We assume each silo contains an unordered collection
of key-value pairs (a multi-map). Then, groupByKey can be implemented as
follows:
\begin{itemize}
\item For each node Ni, the master node creates N SiloRefs.
\item Each node Ni applies a \emph{partitioning function} (example: hash(key) mod N) to
      the key-value pairs in its silo, yielding N (local) silos.
\item Using pumpTo, each pair of silos containing keys of the same range can be combined
      and materialized on the right destination node.
\end{itemize}



\section{Implementation and preliminary experimental results}
\label{sec:impl}

We have developed a prototype\footnote{See \url{https://github.com/heathermiller/f-p}}
of the SCP model in Scala, which builds on our earlier work on
Scala Pickling~\cite{MillerHBO13} and Spores~\cite{MillerHO14}.
The implementation does not require extensions to the Scala language or compiler; it
is developed using the current stable Scala release 2.11.

We have used our implementation to measure the impact of
compile-time-generated serializers~\cite{MillerHBO13} on end-to-end application
performance. In our benchmark application, a group of 4 silos is distributed
across 4 different nodes/JVMs. The silos are first transformed using
\emph{map}, and then using \emph{groupBy}. For an input size of 100'000 ``person''
records, the use of compile-time-generated serializers resulted in
an overall speedup of about 48\% (experiment run on a 2.3 GHz Intel Core i7
with 16 GB RAM under Mac OS X 10.9.5 using Java HotSpot Server 1.8.0-b132).


\section{Related work}

% Recently, there have been efforts (e.g., \cite{}) to make communication APIs
% more typed, however, these approaches typically don't address the problem of
% lower system layers being untyped (it's just the top-level API). Our approach
% in addition aims to actively exploit the additional static type info
% throughout the stack, in particular for optimizing remote communication.

[TODO: rewrite] Related to everything that Spores are related to. But then
also to things like MBrace (ref?).


\section{Conclusion and Future Work}
\label{sec:conclusion}

%------------------------------------------------------------------------------
% Refs:
%
\begin{spacing}{0.9}
\bibliographystyle{plain}
\bibliography{bib}
\end{spacing}

\end{document}
